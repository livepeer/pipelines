---
title: "Create your first Pipeline"
description: ""
icon: "plus"
---

Let's create a fast, useful workflow in ComfyUI and deploy it to ComfyStream.

## Install Custom Nodes for the Workflow

<Note>If you have the official version of the **ComfyUI_TensorRT** custom node installed in your `custom_nodes` directory, you will need to remove it or rename it to `ComfyUI_TensorRT.disabled` to prevent it from being loaded into ComfyUI and interfering with the forked version</Note>

1. Activate the `comfyui` environment before installing nodes

```bash
# Activate your comfyui or comfystream conda environment!
conda activate comfyui
```

2. Install **ComfyUI_TensorRT**, **Torch Compile**, **Depth Anything Tensorrt**
```bash
cd /workspace/comfyRealtime/ComfyStream/custom_nodes
git clone -b quantization_with_controlnet_fixes https://github.com/yondonfu/ComfyUI_TensorRT.git
cd ComfyUI_TensorRT
pip install -r requirements.txt

cd ..
git clone https://github.com/yondonfu/ComfyUI-Torch-Compile
cd ComfyUI-Torch-Compile
pip install -r requirements.txt

cd ..
git clone https://github.com/yuvraj108c/ComfyUI-Depth-Anything-Tensorrt.git
cd ./ComfyUI-Depth-Anything-Tensorrt
pip install -r requirements.txt
```

## Download Models

1. [Dreamshaper 8 DMD](https://huggingface.co/aaronb/dreamshaper-8-dmd-1kstep/tree/main)
```bash
cd /workspace/ComfyUI/models/unet
wget -O "dreamshaper_8_dmd_1kstep.safetensors" "https://huggingface.co/aaronb/dreamshaper-8-dmd-1kstep/resolve/main/diffusion_pytorch_model.safetensors?download=true"
```

2. [Dreamshaper 8 Weights](https://civitai.com/models/4384/dreamshaper)
```bash
cd /workspace/ComfyUI/models/checkpoints
wget "https://civitai.com/api/download/models/4384?token=YOUR_API_KEY" --content-disposition
```

<Note>To obtain an API key, simply go to your account settings and look for the 'API Keys'section and generate a key. See [Download Models](../reference/download-models) for more information on how to download models from CivitAI.</Note>

Alternatively you can download the model from the CivitAI website using the link above
```bash
cd /workspace/ComfyUI/models/checkpoints
wget "https://civitai.com/api/download/models/MODELID?token=YOUR_TOKEN_HERE" --content-disposition
```

3. **ControlNet Models**
```bash
cd /workspace/ComfyUI/models/controlnet
wget https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true --content-disposition
wget https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true --content-disposition
```

## Build TensorRT Engine for Depth Anything

1. Navigate to the `custom_nodes` directory in your ComfyUI workspace

```bash
cd /workspace/comfyRealtime/ComfyUI/custom_nodes
```

2. Clone the repository

```bash
git clone https://github.com/yuvraj108c/ComfyUI-Depth-Anything-Tensorrt.git
cd ./ComfyUI-Depth-Anything-Tensorrt
```

3. Install the node

```bash
conda activate comfyui
pip install -r requirements.txt
```

4. Download the TensorRT onnx file and build the engine

```bash
wget -O depth_anything_vitl14.onnx https://huggingface.co/yuvraj108c/Depth-Anything-2-Onnx/resolve/main/depth_anything_v2_vitb.onnx?download=true --content-disposition
python export_trt.py
```

5. Copy the TensorRT engine file to the ComfyUI `models` directory

```bash
mkdir -p /workspace/comfyRealtime/ComfyUI/models/tensorrt/depth-anything/
mv depth_anything_vitl14-fp16.engine /workspace/comfyRealtime/ComfyUI/models/tensorrt/depth-anything/
```
<Note>If you are copy files from Windows desktop to WSL, you will not be able to find the directory `/workspace/comfyRealtime/ComfyUI/models/tensorrt/depth-anything/`. Instead, use `/workspace/ComfyUI/models/tensorrt/depth-anything/`</Note>


### Build Additional TensorRT Engines in ComfyUI

To build TensorRT engines, you should have the following packages installed in the `comfystream` environment
```bash
pip install tensorrt-cu12-bindings==10.7.0
pip install tensorrt-cu12-libs==10.7.0
```

1. Start ComfyUI and load the following workflow [build-tensorrt.json](../workflow-sample/build-tensorrt.json)
2. In the **ONNX FP8 Export** node, choose a unique filename.
3. Run the workflow to export the ONNX file.
4. Open the following workflow [onnx-import.json](../workflow-sample/onnx-import.json)
5. In the **Load Diffusion Model** node, select the *dreamshaper_8_dmd_1kstep.safetensors* file from the `unet` folder. Set the **weight_dtype** to *default*
6. In the **Select ONNX Model** node, select *dreamshaper8_fp8.onnx*
7. In the STATIC_TRT_MODEL_CONVERSION node, set the filename_prefix to a unique name for this engine. <Warning>If the engine file already exists, an error will occur.</Warning>
8. Run the workflow to generate the TensorRT engine, the output will be stored in the `ComfyUI/output` directory.
<Note>
If you have existing workflows open in other tabs, you must reload the ComfyUI page for the new engine to show in node input field
</Note>

## Load Workflow
Now that the engines are built, we can actually use the workflow!

1. Stop ComfyUI (strike ctrl-c in the terminal that is running ComfyUI)
2. We need to start ComfyUI again, but with an additional flag. The command is as follows:

```bash
python main.py --listen --disable-cuda-malloc
```

3. Now that ComfyUI is started, you can [download the workflow](https://gist.github.com/yondonfu/592a04e075e790981cd401124e0d84e4#file-workflow-json), and drag and drop the file onto the ComfyUI interface.
4. You may be missing the TAESD model, which can be downloaded through Managerâ†’Model Manager.

## Expanding the Workflow

Since Yondon's workflow uses a regular KSampler, it is well integrated with the existing ComfyUI ecosystem. The possibilities are endless! Please see below two example workflows that are built upon Yondon's, which you can further adapt for many other purposes.

*NOTE:* I left the webcam capture node in these workflows. For use in ComfyStream, this would be replaced with either the standard 'load image'node or the 'primary input load image'node.

1. Using **Pose** control net instead of depth. This is done using [this node](https://github.com/yuvraj108c/ComfyUI-YoloNasPose-Tensorrt?utm_source=chatgpt.com), the installation process for which is nearly identical to that of the Depth Anything implementation.

[pose.json](https://prod-files-secure.s3.us-west-2.amazonaws.com/ba5cc718-47bf-4371-9ee3-50bdd7161e7a/5c97396d-f057-46f4-89cf-c00b23dcd3e7/pose.json)

2. The following are not yet working with ComfyStream, as they depend on a brand new core update to ComfyUI. As soon as we update ComfyStream (impending), they should work. You can still mess with them in ComfyUI for now:
- LoRAs! The ones I used are made by [this creator](https://civitai.com/user/RalFinger/models) on CivitAI. Look for sd1.5 models, he has hundreds.
    - Solid LoRA
    
[solid_lora.json](https://prod-files-secure.s3.us-west-2.amazonaws.com/ba5cc718-47bf-4371-9ee3-50bdd7161e7a/59d3860a-99f3-42e3-b553-1cf5cc3dfb6b/solid_lora.json)

- Regional LoRA and prompt application using the new hook system

[regional_lora.json](https://prod-files-secure.s3.us-west-2.amazonaws.com/ba5cc718-47bf-4371-9ee3-50bdd7161e7a/169a5924-9983-421e-a801-7d813ee93d58/regional_lora.json)

- Moving LoRA

[moving_lora.json](https://prod-files-secure.s3.us-west-2.amazonaws.com/ba5cc718-47bf-4371-9ee3-50bdd7161e7a/b9c850d8-f489-4ca4-9835-0040173224cc/moving_lora.json)

## Running The Workflow in ComfyStream

To run TensorRT engines, you should have the following packages installed in the `comfystream` environment
```bash
pip install tensorrt-cu12-bindings==10.7.0
pip install tensorrt-cu12-libs==10.7.0
```

Since we have added new nodes in ComfyUI, proceed to [Install nodes in ComfyStream](technical/reference/custom-node-installation#install-nodes-in-comfystream), before running the workflow in ComfyStream.

If you have two environments (one for ComfyUI and one for ComfyStream), you'll need to copy your generated TensorRT model with the command below:
```bash
cp -r /workspace/ComfyUI/output/tensorrt /workspace/comfyRealtime/ComfyUI/output/
```

