---
title: 'Available ComfyUI nodes'
description: 'This guide covers the available nodes and requirements for creating real-time video pipelines using ComfyUI with Livepeer'
icon: 'server'
---

## Required Video Input/Output Nodes
These nodes are required for creating real-time video pipelines

<AccordionGroup>
  <Accordion header="ComfyStream" title="ComfyStream" defaultOpen>
    - **Input:**
      - Video stream URL or device ID
      - Optional configuration parameters
    - **Output:**
      - RGB frame tensor (3, H, W)
      - Frame metadata (timestamp, index)
    - **Performance Requirements:**
      - Frame processing time: < 5ms
      - VRAM usage: < 500MB
      - Buffer size: ≤ 2 frames
      - Supported formats: RTMP, WebRTC, V4L2
    - **Best Practices:**
      - Set fixed frame rate
  <div style={{ float: 'right', textDecoration: 'none' }}>
    [GitHub Link](https://github.com/yondonfu/comfystream/)
  </div>
  </Accordion>
</AccordionGroup>

## Inference Nodes
Nodes for analyzing video frames in real-time. They can be used for tasks like object detection, segmentation, and depth estimation

<AccordionGroup>
  <Accordion header="DepthAnythingTensorRT" title="Depth Anything TensorRT">
  <Tabs>
    <TabItem value="Overview" label="Overview">
      Overview content goes here.
    </TabItem>
    <TabItem value="Details" label="Details">
      Details content goes here.
    </TabItem>
    <TabItem value="Examples" label="Examples">
      Examples content goes here.
    </TabItem>
  </Tabs>

    - **Input:** RGB frame (3, H, W)
    - **Output:** Depth map (1, H, W)
    - **Performance Requirements:**
      - Inference time: < 20ms
      - VRAM usage: 2GB
      - Batch size: 1
    - **Best Practices:**
      - Place early in workflow
      - Cache results for static scenes
      - Use lowest viable resolution
    <div style={{ float: 'right', textDecoration: 'none' }}>
      [GitHub Link](https://github.com/yuvraj108c/ComfyUI-Depth-Anything-Tensorrt)
    </div>
  </Accordion>
  <Accordion header="Segment Anything 2" title="Segment Anything 2">
    - **Input:** RGB frame (3, H, W)
    - **Output:** Segmentation mask (1, H, W)
    - **Performance Requirements:**
      - Inference time: < 30ms
      - VRAM usage: 3GB
      - Batch size: 1
    - **Best Practices:**
      - Cache static masks
      - Use mask erosion for stability
      - Implement confidence thresholding
    <div style={{ float: 'right', textDecoration: 'none' }}>
      [GitHub Link](https://github.com/pschroedl/ComfyUI-SAM2-Realtime)
    </div>
  </Accordion>
  <Accordion header="Florence2" title="Florence2">
    - **Input:** RGB frame (3, H, W)
    - **Output:** Feature vector (1, 512)
    - **Performance Requirements:**
      - Inference time: < 15ms
      - VRAM usage: 1GB
      - Batch size: 1
    - **Best Practices:**
      - Cache embeddings for known references
      - Use cosine similarity for matching
      - Implement feature vector normalization
    <div style={{ float: 'right', textDecoration: 'none' }}>
      [GitHub Link](https://github.com/ad-astra-video/ComfyUI-Florence2-Vision)
    </div>
  </Accordion>
</AccordionGroup>

## Generative and Control Nodes
Nodes for applying visual effects and controlling video content in real-time
<AccordionGroup>
  <Accordion header="StreamDiffusion" title="StreamDiffusion">
    - **Input:** RGB frame (3, H, W)
    - **Output:** Feature vector (1, 512)
    <div style={{ float: 'right', textDecoration: 'none' }}>
      [GitHub Link](https://github.com/pschroedl/ComfyUI-StreamDiffusion)
    </div>
  </Accordion>
  <Accordion header="LivePortraitKJ" title="LivePortraitKJ">
    - **Input:**
      - Source image (3, H, W)
      - Driving frame (3, H, W)
    - **Output:** Animated frame (3, H, W)
    - **Performance Requirements:**
      - Inference time: < 50ms
      - VRAM usage: 4GB
      - Batch size: 1
    - **Best Practices:**
      - Pre-process source images
      - Implement motion smoothing
      - Cache facial landmarks
    <div style={{ float: 'right', textDecoration: 'none' }}>
      [GitHub Link](https://github.com/kijai/ComfyUI-LivePortraitKJ)
    </div>
  </Accordion>
  <Accordion header="ComfyUI Diffusers" title="ComfyUI Diffusers">
    - **Input:**
      - Conditioning tensor
      - Latent tensor
    - **Output:** Generated frame (3, H, W)
    - **Performance Requirements:**
      - Inference time: < 50ms
      - VRAM usage: 4GB
      - Maximum steps: 20
    - **Best Practices:**
      - Use TensorRT optimization
      - Implement denoising strength control
      - Cache conditioning tensors
    <div style={{ float: 'right', textDecoration: 'none' }}>
      [GitHub Link](https://github.com/Limitex/ComfyUI-Diffusers)
    </div>
  </Accordion>
  <Accordion header="ComfyUI-load-image-from-url Node" title="ComfyUI-load-image-from-url Node">
    - **Input:**
      - Control signal
      - Target tensor
    - **Output:** Controlled tensor
    - **Performance Requirements:**
      - Inference time: < 30ms
      - VRAM usage: 2GB
      - Resolution: ≤ 512
    - **Best Practices:**
      - Use adaptive conditioning
      - Implement strength scheduling
      - Cache control signals
      <div style={{ float: 'right', textDecoration: 'none' }}>
        [GitHub Link](https://github.com/tsogzark/ComfyUI-load-image-from-url.git)
      </div>
  </Accordion>
</AccordionGroup>

## Supporting Nodes

These nodes are used to provide inputs, prompts and other supporting functions for the video pipeline

<AccordionGroup>
  <Accordion header="K Sampler" title="K Sampler">
    - **Input:**
      - Latent tensor
      - Conditioning
    - **Output:** Sampled latent
    - **Performance Requirements:**
      - Maximum steps: 20
      - VRAM usage: 2GB
      - Scheduler: euler_ancestral
    - **Best Practices:**
      - Use adaptive step sizing
      - Cache conditioning tensors
  </Accordion>
  <Accordion header="Prompt Control" title="Prompt Control">
    - **Input:** Text prompts
    - **Output:** Conditioning tensors
    - **Performance Requirements:**
      - Processing time: < 5ms
      - VRAM usage: minimal
    - **Best Practices:**
      - Cache common prompts
      - Use consistent style tokens
      - Implement prompt weighting
  </Accordion>
  <Accordion header="VAE" title="VAE">
    - **Input:** Latent tensor
    - **Output:** RGB frame
    - **Performance Requirements:**
      - Inference time: < 10ms
      - VRAM usage: 1GB
      - Tile size: 512
    - **Best Practices:**
      - Use tiling for large frames
      - Implement half-precision
      - Cache common latents
  </Accordion>
  <Accordion header="IPAdapter" title="IPAdapter">
    - **Input:**
      - Reference image
      - Target tensor
    - **Output:** Conditioned tensor
    - **Performance Requirements:**
      - Inference time: < 20ms
      - VRAM usage: 2GB
      - Reference resolution: ≤ 512x512
    - **Best Practices:**
      - Cache reference embeddings
      - Use consistent weights
      - Implement cross-attention
  </Accordion>
  <Accordion header="Cache Nodes" title="Cache Nodes">
    - **Input:** Any tensor
    - **Output:** Cached tensor
    - **Performance Requirements:**
      - Access time: < 1ms
      - Maximum size: 2GB
      - Cache type: GPU
    - **Best Practices:**
      - Implement LRU eviction
      - Monitor cache pressure
      - Clear on scene changes
  </Accordion>
  <Accordion header="ControlNet" title="ControlNet">
    - **Input:**
      - Control signal
      - Target tensor
    - **Output:** Controlled tensor
    - **Performance Requirements:**
      - Inference time: < 30ms
      - VRAM usage: 2GB
      - Resolution: ≤ 512
    - **Best Practices:**
      - Use adaptive conditioning
      - Implement strength scheduling
      - Cache control signals
  </Accordion>
   
</AccordionGroup>

## Default Nodes

  All default nodes that ship with ComfyUI are available. The list below is subject to change.
  <Expandable title="Default ComfyUI Nodes">
  - AddNoise
  - AlignYourStepsScheduler
  - BasicGuider
  - BasicScheduler
  - BetaSamplingScheduler
  - CFGGuider
  - CLIPAttentionMultiply
  - CLIPLoader
  - CLIPMergeAdd
  - CLIPMergeSimple
  - CLIPMergeSubtract
  - CLIPSave
  - CLIPSetLastLayer
  - CLIPTextEncode
  - CLIPTextEncodeControlnet
  - CLIPTextEncodeFlux
  - CLIPTextEncodeHunyuanDiT
  - CLIPTextEncodeSD3
  - CLIPTextEncodeSDXL
  - CLIPTextEncodeSDXLRefiner
  - CLIPVisionEncode
  - CLIPVisionLoader
  - Canny
  - CheckpointLoader
  - CheckpointLoaderSimple
  - CheckpointSave
  - ConditioningAverage
  - ConditioningCombine
  - ConditioningConcat
  - ConditioningSetArea
  - ConditioningSetAreaPercentage
  - ConditioningSetAreaStrength
  - ConditioningSetMask
  - ConditioningSetTimestepRange
  - ConditioningZeroOut
  - ControlNetApply
  - ControlNetApplyAdvanced
  - ControlNetApplySD3
  - ControlNetInpaintingAliMamaApply
  - ControlNetLoader
  - CropMask
  - DiffControlNetLoader
  - DifferentialDiffusion
  - DiffusersLoader
  - DisableNoise
  - DualCFGGuider
  - DualCLIPLoader
  - EmptyHunyuanLatentVideo
  - EmptyImage
  - EmptyLTXVLatentVideo
  - EmptyLatentAudio
  - EmptyLatentImage
  - EmptyMochiLatentVideo
  - EmptySD3LatentImage
  - ExponentialScheduler
  - FeatherMask
  - FlipSigmas
  - FluxGuidance
  - FreeU
  - FreeU_V2
  - GITSScheduler
  - GLIGENLoader
  - GLIGENTextBoxApply
  - GrowMask
  - HyperTile
  - HypernetworkLoader
  - ImageBatch
  - ImageBlend
  - ImageBlur
  - ImageColorToMask
  - ImageCompositeMasked
  - ImageCrop
  - ImageFromBatch
  - ImageInvert
  - ImageOnlyCheckpointLoader
  - ImageOnlyCheckpointSave
  - ImagePadForOutpaint
  - ImageQuantize
  - ImageScale
  - ImageScaleBy
  - ImageScaleToTotalPixels
  - ImageSharpen
  - ImageToMask
  - ImageUpscaleWithModel
  - InpaintModelConditioning
  - InstructPixToPixConditioning
  - InvertMask
  - JoinImageWithAlpha
  - KSampler
  - KSamplerAdvanced
  - KSamplerSelect
  - KarrasScheduler
  - LTXVConditioning
  - LTXVImgToVideo
  - LTXVScheduler
  - LaplaceScheduler
  - LatentAdd
  - LatentApplyOperation
  - LatentApplyOperationCFG
  - LatentBatch
  - LatentBatchSeedBehavior
  - LatentBlend
  - LatentComposite
  - LatentCompositeMasked
  - LatentCrop
  - LatentFlip
  - LatentFromBatch
  - LatentInterpolate
  - LatentMultiply
  - LatentOperationSharpen
  - LatentOperationTonemapReinhard
  - LatentRotate
  - LatentSubtract
  - LatentUpscale
  - LatentUpscaleBy
  - Load3D
  - Load3DAnimation
  - LoadAudio
  - LoadImage
  - LoadImageMask
  - LoadLatent
  - LoraLoader
  - LoraLoaderModelOnly
  - LoraSave
  - Mahiro
  - MaskComposite
  - MaskToImage
  - ModelMergeAdd
  - ModelMergeAuraflow
  - ModelMergeBlocks
  - ModelMergeFlux1
  - ModelMergeLTXV
  - ModelMergeMochiPreview
  - ModelMergeSD1
  - ModelMergeSD2
  - ModelMergeSD35_Large
  - ModelMergeSD3_2B
  - ModelMergeSDXL
  - ModelMergeSimple
  - ModelMergeSubtract
  - ModelSamplingAuraFlow
  - ModelSamplingContinuousEDM
  - ModelSamplingContinuousV
  - ModelSamplingDiscrete
  - ModelSamplingFlux
  - ModelSamplingLTXV
  - ModelSamplingSD3
  - ModelSamplingStableCascade
  - ModelSave
  - Morphology
  - PatchModelAddDownscale
  - PerpNeg
  - PerpNegGuider
  - PerturbedAttentionGuidance
  - PhotoMakerEncode
  - PhotoMakerLoader
  - PolyexponentialScheduler
  - PorterDuffImageComposite
  - PreviewAudio
  - PreviewImage
  - RandomNoise
  - RebatchImages
  - RebatchLatents
  - RepeatImageBatch
  - RepeatLatentBatch
  - RescaleCFG
  - SDTurboScheduler
  - SD_4XUpscale_Conditioning
  - SV3D_Conditioning
  - SVD_img2vid_Conditioning
  - SamplerCustom
  - SamplerCustomAdvanced
  - SamplerDPMAdaptative
  - SamplerDPMPP_2M_SDE
  - SamplerDPMPP_2S_Ancestral
  - SamplerDPMPP_3M_SDE
  - SamplerDPMPP_SDE
  - SamplerEulerAncestral
  - SamplerEulerAncestralCFGPP
  - SamplerEulerCFGpp
  - SamplerLCMUpscale
  - SamplerLMS
  - SaveAnimatedPNG
  - SaveAnimatedWEBP
  - SaveAudio
  - SaveImage
  - SaveImageWebsocket
  - SaveLatent
  - SelfAttentionGuidance
  - SetLatentNoiseMask
  - SetUnionControlNetType
  - SkipLayerGuidanceDiT
  - SkipLayerGuidanceSD3
  - SolidMask
  - SplitImageWithAlpha
  - SplitSigmas
  - SplitSigmasDenoise
  - StableCascade_EmptyLatentImage
  - StableCascade_StageB_Conditioning
  - StableCascade_StageC_VAEEncode
  - StableCascade_SuperResolutionControlnet
  - StableZero123_Conditioning
  - StableZero123_Conditioning_Batched
  - StubConstantImage
  - StubFloat
  - StubImage
  - StubInt
  - StubMask
  - StyleModelApply
  - StyleModelLoader
  - TestAccumulateNode
  - TestAccumulationGetItemNode
  - TestAccumulationGetLengthNode
  - TestAccumulationHeadNode
  - TestAccumulationSetItemNode
  - TestAccumulationTailNode
  - TestAccumulationToListNode
  - TestBoolOperationNode
  - TestCustomIsChanged
  - TestCustomValidation1
  - TestCustomValidation2
  - TestCustomValidation3
  - TestCustomValidation4
  - TestCustomValidation5
  - TestDynamicDependencyCycle
  - TestExecutionBlocker
  - TestFloatConditions
  - TestForLoopClose
  - TestForLoopOpen
  - TestIntConditions
  - TestIntMathOperation
  - TestIsChangedWithConstants
  - TestLazyMixImages
  - TestListToAccumulationNode
  - TestMakeListNode
  - TestMixedExpansionReturns
  - TestStringConditions
  - TestToBoolNode
  - TestVariadicAverage
  - TestWhileLoopClose
  - TestWhileLoopOpen
  - ThresholdMask
  - TomePatchModel
  - TorchCompileModel
  - TripleCLIPLoader
  - UNETLoader
  - UNetCrossAttentionMultiply
  - UNetSelfAttentionMultiply
  - UNetTemporalAttentionMultiply
  - UpscaleModelLoader
  - VAEDecode
  - VAEDecodeAudio
  - VAEDecodeTiled
  - VAEEncode
  - VAEEncodeAudio
  - VAEEncodeForInpaint
  - VAEEncodeTiled
  - VAELoader
  - VAESave
  - VPScheduler
  - VideoLinearCFGGuidance
  - VideoTriangleCFGGuidance
  - WebcamCapture
  - unCLIPCheckpointLoader
  - unCLIPConditioning


  </Expandable>



