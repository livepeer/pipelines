---
title: "Install ComfyStream Locally"
description: "Run ComfyStream on your local machine with a GPU"
icon: "computer"
---

## Prerequisites

Before installing ComfyStream locally, ensure you have:

<Tabs>
  <Tab title="Linux">
    Install the necessary software:
    - [Docker Engine](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)
    - [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-12-6-3-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu)
    - [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
  </Tab>

  <Tab title="Windows">
    Install the following dependencies:
    - [Docker Desktop for Windows](https://docs.docker.com/get-started/introduction/get-docker-desktop/)
    - [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-12-6-3-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local)
  </Tab>
</Tabs>

## Installation Steps

### Create Directories to Store Models

1. Choose a location to store AI models and output files from ComfyUI:
  - Models: `~/models/ComfyUI--models`
  - Output: `~/models/ComfyUI--output`

<CodeGroup>
```bash Linux
mkdir -p ~/models/ComfyUI--models ~/models/ComfyUI--output  
```
```batch Command Line
mkdir %USERPROFILE%\models\ComfyUI--models %USERPROFILE%\models\ComfyUI--output
```
```powershell PowerShell
New-Item -ItemType Directory -Path "$env:USERPROFILE\models\ComfyUI--models", "$env:USERPROFILE\models\ComfyUI--output"
```
</CodeGroup>

### Download and Run

1. Download the docker image:
```bash
docker pull livepeer/comfyui-base:server
```

2. Run the container:
<Info>
If using Windows, ensure Docker Desktop is running first
</Info>

<CodeGroup>
```bash Linux
docker run -it --gpus all \
-p 8188:8188 \
-p 8888:8888 \
-p 5678:5678 \
-p 3000:3000 \
-v ~/models/ComfyUI--models:/ComfyUI/models \
-v ~/models/ComfyUI--output:/ComfyUI/output \
livepeer/comfyui-base:server --download-models --build-engines --server
```
```batch Command Line
docker run -it --gpus all ^
-p 8188:8188 ^
-p 8888:8888 ^
-p 5678:5678 ^
-p 3000:3000 ^
-v %USERPROFILE%\models\ComfyUI--models:/ComfyUI/models ^
-v %USERPROFILE%\models\ComfyUI--output:/ComfyUI/output ^
livepeer/comfyui-base:server --download-models --build-engines --server
```
```powershell Powershell
docker run -it --gpus all `
-p 8188:8188 `
-p 8888:8888 `
-p 5678:5678 `
-p 3000:3000 `
-v $env:USERPROFILE\models\ComfyUI--models:/ComfyUI/models `
-v $env:USERPROFILE\models\ComfyUI--output:/ComfyUI/output `
livepeer/comfyui-base:server --download-models --build-engines --server
```
</CodeGroup>

The `--download-models` and `--build-engines` flags will download required models and build TensorRT engines. This process may take some time depending on your network connection and GPU hardware and is only needed on the first run or when adding new models.

<Note>
Engine files must be compiled on the same GPU hardware/architecture they will be used on.
</Note>

## Access ComfyUI and ComfyStream

The `--server` flag starts ComfyUI, ComfyStream and UI automatically. Access the services at:
- ComfyUI: [http://localhost:8188](http://localhost:8188)
- ComfyStream: [http://localhost:8888](http://localhost:8888)  
- UI: [http://localhost:3000](http://localhost:3000)
